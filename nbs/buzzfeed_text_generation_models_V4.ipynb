{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "buzzfeed-text-generation-models-V4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LM1kKz-N6Wpe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import tensorflow.keras.utils as ku\n",
        "\n",
        "tf.random.set_seed(2021)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/buzzfeed_headlines.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "02n5NCI064f7",
        "outputId": "b2b449bc-fa8b-4836-9a43-20245c58342f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3f5f640f-ee97-42ca-ae27-59dbd9c22c20\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>description</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CBS Films\\r\\nI couldn't find the second video ...</td>\n",
              "      <td>Warning: This is gonna get pretty cute.View En...</td>\n",
              "      <td>16 Wholesome Details From TV And Movies That S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HBO Max\\r\\n\"When I was working with Alexa last...</td>\n",
              "      <td>\"I needed to have something different in my ey...</td>\n",
              "      <td>Jacob Elordi Has Been Living The Dream Filming...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Honestly working from home HAS been a game ch...</td>\n",
              "      <td>\"Work will never love you back.\"View Entire Po...</td>\n",
              "      <td>People Are Sharing Things That Helped Them Get...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mike Marsland / WireImage via Getty Images\\r\\n...</td>\n",
              "      <td>You really can't win in Hollywood.View Entire ...</td>\n",
              "      <td>7 Actors Who Were Considered Too \"Ugly\" For A ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jane Barlow - Pa Images / PA Images via Getty ...</td>\n",
              "      <td>\"It would have been a money spinner.\"</td>\n",
              "      <td>Brian Cox Says He Turned Down A Role In “Pirat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f5f640f-ee97-42ca-ae27-59dbd9c22c20')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f5f640f-ee97-42ca-ae27-59dbd9c22c20 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f5f640f-ee97-42ca-ae27-59dbd9c22c20');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             content  ...                                              title\n",
              "0  CBS Films\\r\\nI couldn't find the second video ...  ...  16 Wholesome Details From TV And Movies That S...\n",
              "1  HBO Max\\r\\n\"When I was working with Alexa last...  ...  Jacob Elordi Has Been Living The Dream Filming...\n",
              "2  \"Honestly working from home HAS been a game ch...  ...  People Are Sharing Things That Helped Them Get...\n",
              "3  Mike Marsland / WireImage via Getty Images\\r\\n...  ...  7 Actors Who Were Considered Too \"Ugly\" For A ...\n",
              "4  Jane Barlow - Pa Images / PA Images via Getty ...  ...  Brian Cox Says He Turned Down A Role In “Pirat...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['content', 'description'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ULaofkde7LB-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_text = df['title'].tolist()\n",
        "print(title_text[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blxE5YJD7rTl",
        "outputId": "2b906379-8234-4857-801f-77b640f40353"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['16 Wholesome Details From TV And Movies That Show They Were Quite Literally Made With Love', 'Jacob Elordi Has Been Living The Dream Filming Passionate Love Scenes With Sydney Sweeney On \"Euphoria,\" And He Knows It', \"People Are Sharing Things That Helped Them Get A Better Work-Life Balance, And It's So Refreshing\", '7 Actors Who Were Considered Too \"Ugly\" For A Role, And 8 Who Weren\\'t Ugly Enough', 'Brian Cox Says He Turned Down A Role In “Pirates Of The Caribbean”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = ''.join(w for w in text if w not in string.punctuation).lower()\n",
        "  text = text.encode('utf8').decode('ascii', 'ignore')\n",
        "\n",
        "  return text\n",
        "\n",
        "corpus = [clean_text(w) for w in title_text]"
      ],
      "metadata": {
        "id": "cT18O6ot7Q7E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1PxZkHl7j0P",
        "outputId": "9f413331-1d9a-40c3-adb4-3c5b7be94ac4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['16 wholesome details from tv and movies that show they were quite literally made with love', 'jacob elordi has been living the dream filming passionate love scenes with sydney sweeney on euphoria and he knows it', 'people are sharing things that helped them get a better worklife balance and its so refreshing', '7 actors who were considered too ugly for a role and 8 who werent ugly enough', 'brian cox says he turned down a role in pirates of the caribbean', 'eat at this dessert buffet like a toddler at a birthday party and ill tell you which bsc character you remind me of', 'directors must really love these 17 pairs of actors together since they keep getting cast as costars', 'all of the major pop culture happenings this week from celebrity tributes to new trailers', 'aussies are sharing how their experiences overseas have changed their view on australia and the responses are fascinating', 'telling you not to discuss your pay with coworkers and 18 other red flags in the workplace', 'if you want results check out these 31 products with impressive before and after photos', 'breakfast is a construct and 20 other food opinions that people believe with their whole chest', '12 sitcom characters whod excel in the pandemic 14 whod handle it alright and 12 whowouldnt do well', 'britney spears walked back her comments about jamie lynn being scum and added that she was just obviously hurt by her', 'former employees are anonymously sharing company secrets and this is some tea yall']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "def get_sequences(corpus):\n",
        "  # create tokens\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "  # convert to sequences\n",
        "  input_sequences = []\n",
        "  for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "      n_gram_sequence = token_list[:i+1]\n",
        "      input_sequences.append(n_gram_sequence)\n",
        "\n",
        "  return input_sequences, total_words\n",
        "\n",
        "inp_sequences, total_words = get_sequences(corpus)\n",
        "inp_sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDLn1ZqU783p",
        "outputId": "917e9466-dcd5-4336-c703-e52e6115a03a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[130, 413],\n",
              " [130, 413, 204],\n",
              " [130, 413, 204, 18],\n",
              " [130, 413, 204, 18, 63],\n",
              " [130, 413, 204, 18, 63, 2],\n",
              " [130, 413, 204, 18, 63, 2, 97],\n",
              " [130, 413, 204, 18, 63, 2, 97, 8],\n",
              " [130, 413, 204, 18, 63, 2, 97, 8, 118],\n",
              " [130, 413, 204, 18, 63, 2, 97, 8, 118, 23],\n",
              " [130, 413, 204, 18, 63, 2, 97, 8, 118, 23, 44]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = Tokenizer()\n",
        "\n",
        "# def get_sequence_of_tokens(corpus):\n",
        "#   # tokenization\n",
        "#   tokenizer.fit_on_texts(corpus)\n",
        "#   total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "#   # convert data to sequence of tokens\n",
        "#   input_sequences = []\n",
        "#   for line in corpus:\n",
        "#     token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "#     for i in range(1, len(token_list)):\n",
        "#       n_gram_sequence = token_list[:i+1]\n",
        "#       input_sequences.append(n_gram_sequence)\n",
        "#   return input_sequences, total_words\n",
        "\n",
        "# inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "# inp_sequences[:10]"
      ],
      "metadata": {
        "id": "bOYQtm-fU_nW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding the sequences\n",
        "def generate_padded_sequences(input_sequences):\n",
        "  max_sequence_len = max([len(x) for x in input_sequences])\n",
        "  input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "  predictors, label = input_sequences[:,:-1], input_sequences[:, -1]\n",
        "  label = ku.to_categorical(label, num_classes=total_words)\n",
        "  return predictors, label, max_sequence_len\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ],
      "metadata": {
        "id": "4H-rajtm9DeS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "  input_len = max_sequence_len - 1\n",
        "  model = Sequential()\n",
        "\n",
        "  # Input Embedding Layer\n",
        "  model.add(Embedding(total_words, 50, input_length=input_len))\n",
        "\n",
        "  # Add a Hidden Layer - LSTM Layer\n",
        "  model.add(LSTM(128, return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  # Second Hidden Layer\n",
        "  model.add(LSTM(128))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  # Add Output Layer\n",
        "  model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62weMkM1OCSm",
        "outputId": "8a0ccc29-966f-4681-e98e-5570b3693a5c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 32, 50)            174800    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32, 128)           91648     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32, 128)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3496)              450984    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 849,016\n",
            "Trainable params: 849,016\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlKPOIT5Q6m6",
        "outputId": "7d050aab-f58f-4813-9972-3944f2110008"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "433/433 [==============================] - 8s 9ms/step - loss: 7.1431\n",
            "Epoch 2/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 6.7363\n",
            "Epoch 3/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 6.6174\n",
            "Epoch 4/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 6.5225\n",
            "Epoch 5/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 6.4060\n",
            "Epoch 6/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 6.2827\n",
            "Epoch 7/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 6.1581\n",
            "Epoch 8/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 6.0347\n",
            "Epoch 9/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 5.9197\n",
            "Epoch 10/100\n",
            "433/433 [==============================] - 5s 10ms/step - loss: 5.8054\n",
            "Epoch 11/100\n",
            "433/433 [==============================] - 4s 10ms/step - loss: 5.6986\n",
            "Epoch 12/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 5.5938\n",
            "Epoch 13/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 5.4832\n",
            "Epoch 14/100\n",
            "433/433 [==============================] - 4s 10ms/step - loss: 5.3754\n",
            "Epoch 15/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 5.2644\n",
            "Epoch 16/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 5.1552\n",
            "Epoch 17/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 5.0498\n",
            "Epoch 18/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.9364\n",
            "Epoch 19/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.8328\n",
            "Epoch 20/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.7277\n",
            "Epoch 21/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.6189\n",
            "Epoch 22/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.5162\n",
            "Epoch 23/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.4116\n",
            "Epoch 24/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.3132\n",
            "Epoch 25/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.2154\n",
            "Epoch 26/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.1229\n",
            "Epoch 27/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 4.0370\n",
            "Epoch 28/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.9521\n",
            "Epoch 29/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.8729\n",
            "Epoch 30/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.7925\n",
            "Epoch 31/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.7033\n",
            "Epoch 32/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.6489\n",
            "Epoch 33/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.5692\n",
            "Epoch 34/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.5116\n",
            "Epoch 35/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.4418\n",
            "Epoch 36/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.3663\n",
            "Epoch 37/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.3103\n",
            "Epoch 38/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.2669\n",
            "Epoch 39/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.1976\n",
            "Epoch 40/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.1436\n",
            "Epoch 41/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.0912\n",
            "Epoch 42/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 3.0414\n",
            "Epoch 43/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.9967\n",
            "Epoch 44/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.9518\n",
            "Epoch 45/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.9029\n",
            "Epoch 46/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.8613\n",
            "Epoch 47/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.8314\n",
            "Epoch 48/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.7877\n",
            "Epoch 49/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.7385\n",
            "Epoch 50/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.6962\n",
            "Epoch 51/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.6652\n",
            "Epoch 52/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.6366\n",
            "Epoch 53/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.6010\n",
            "Epoch 54/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.5595\n",
            "Epoch 55/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.5269\n",
            "Epoch 56/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.4984\n",
            "Epoch 57/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.4641\n",
            "Epoch 58/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.4290\n",
            "Epoch 59/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.4045\n",
            "Epoch 60/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.3939\n",
            "Epoch 61/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.3527\n",
            "Epoch 62/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.3147\n",
            "Epoch 63/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.2993\n",
            "Epoch 64/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.2633\n",
            "Epoch 65/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.2565\n",
            "Epoch 66/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.2277\n",
            "Epoch 67/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.1919\n",
            "Epoch 68/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.1742\n",
            "Epoch 69/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.1506\n",
            "Epoch 70/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.1321\n",
            "Epoch 71/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.1033\n",
            "Epoch 72/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.0847\n",
            "Epoch 73/100\n",
            "433/433 [==============================] - 5s 11ms/step - loss: 2.0671\n",
            "Epoch 74/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.0372\n",
            "Epoch 75/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 2.0160\n",
            "Epoch 76/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.9956\n",
            "Epoch 77/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.9732\n",
            "Epoch 78/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.9616\n",
            "Epoch 79/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.9540\n",
            "Epoch 80/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.9146\n",
            "Epoch 81/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.9050\n",
            "Epoch 82/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.8802\n",
            "Epoch 83/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.8656\n",
            "Epoch 84/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.8580\n",
            "Epoch 85/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.8395\n",
            "Epoch 86/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.8131\n",
            "Epoch 87/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.7993\n",
            "Epoch 88/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.7883\n",
            "Epoch 89/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.7680\n",
            "Epoch 90/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.7501\n",
            "Epoch 91/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.7420\n",
            "Epoch 92/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.7105\n",
            "Epoch 93/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.7202\n",
            "Epoch 94/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.6780\n",
            "Epoch 95/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.6790\n",
            "Epoch 96/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.6472\n",
            "Epoch 97/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.6297\n",
            "Epoch 98/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.6303\n",
            "Epoch 99/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.6321\n",
            "Epoch 100/100\n",
            "433/433 [==============================] - 4s 9ms/step - loss: 1.6174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "  for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0))\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "  return seed_text.title()"
      ],
      "metadata": {
        "id": "wLENmzToRFao"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"taylor swift didn't\", 12, model, max_sequence_len))\n",
        "print(generate_text(\"all the things you wish\", 15, model, max_sequence_len))\n",
        "print(generate_text(\"the best actors that\", 20, model, max_sequence_len))\n",
        "print(generate_text(\"things you wish you had\", 10, model, max_sequence_len))\n",
        "print(generate_text(\"celebrities you never knew\", 8, model, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgRGnFIsSK8e",
        "outputId": "aaec644e-4e4a-4a0b-c896-609487e047cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taylor Swift Didn'T Revealed That Her Grandmother Tippy Hedrens Career Was Ruined Because There In\n",
            "All The Things You Wish Three Squirrel For Halloween 2021 And Its Actually Super Concerning About His Skinny Music Awards\n",
            "The Best Actors That Her Week From Find Out Which Things From Both Beyond Reigning Fire Pickup Really Really More History At All Costs\n",
            "Things You Wish You Had About Sebastian Stans Talking Penis In Pam Tommy Army Heres\n",
            "Celebrities You Never Knew To Buy To Be The Mess From Thank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8kA8Z9QTTAKD"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}